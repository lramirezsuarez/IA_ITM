{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "7. Red Neuronal y Backpropagation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxo9AjyBdoBp",
        "colab_type": "text"
      },
      "source": [
        "<hr>\n",
        "\n",
        "<center>\n",
        "<img align=\"center\" src=\"http://www.redttu.edu.co/es/wp-content/uploads/2016/01/itm.png\" width=\"600\"/>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8XEUhoxdoBu",
        "colab_type": "text"
      },
      "source": [
        "<hr>\n",
        "\n",
        "#### Pedro Atencio Ortiz - 2020 (pedroatencio@itm.edu.co)\n",
        "\n",
        "\n",
        "# Red Neuronal y Backpropagation (descenso del gradiente generalizado)\n",
        "\n",
        "En este notebook abordaremos los siguientes tópicos:\n",
        "\n",
        "- Notación.\n",
        "- Forward propagation.\n",
        "- Backpropagation.\n",
        "- Ejemplo completo (XOR)\n",
        "- Ejemplo con dataset sintético.\n",
        "- Errores y funciones de activación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLP5pm8KdoBy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Funciones utilitarias\n",
        "\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generate_data(data_type, noise=0.2):\n",
        "    \n",
        "    np.random.seed(0)\n",
        "    if data_type == 'moons':\n",
        "        X, Y = datasets.make_moons(200, noise=noise)\n",
        "    elif data_type == 'circles':\n",
        "        X, Y = sklearn.datasets.make_circles(200, noise=noise)\n",
        "    elif data_type == 'blobs':\n",
        "        X, Y = sklearn.datasets.make_blobs(centers=2, cluster_std=noise)\n",
        "    return X, Y\n",
        "\n",
        "def visualize_lr(W1, b1, W2, b2, X, Y):\n",
        "    X = np.copy(X.T)\n",
        "    # Set min and max values and give it some padding\n",
        "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "    h = 0.01\n",
        "    # Generate a grid of points with distance h between them\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    # Predict the function value for the whole gid\n",
        "    Z = predict_multilayer(W1,b1,W2,b2,np.c_[xx.ravel(), yy.ravel()].T)\n",
        "\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    # Plot the contour and training examples\n",
        "    plt.figure(figsize=(7,5))\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.bone)\n",
        "    \n",
        "    color = ['blue' if y == 1 else 'red' for y in np.squeeze(Y)]\n",
        "    plt.scatter(X[:,0], X[:,1], color=color)\n",
        "    \n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fXEpXUsdoB_",
        "colab_type": "text"
      },
      "source": [
        "<hr>\n",
        "\n",
        "## 3.1. Notación\n",
        "\n",
        "Con el objetivo de poder trabajar un mecanismo general de entrenamiento de una secuencia de regresores, llamados de ahora en adelante red neuronal, necesitamos primero definir una notación genérica para el dataset, parámetros de la red neuronal y resultados de esta. Analicemos la siguiente figura:\n",
        "\n",
        "<img align=\"center\" src=\"https://github.com/psatencio/intro_keras/blob/master/figures/layered_regresor_general.png?raw=true\" width=\"500\"/>\n",
        "\n",
        "Nótese que tanto el conjunto de datos de entrada $X$ como los pesos $W$ y demás elementos pueden definirse como matrices y arreglos, que al ser operados mediante vectorization / broadcasting, nos permiten simplificar el proceso de construcción y cálculo.\n",
        "\n",
        "A partir de lo anterior, definamos los siguientes elementos:\n",
        "\n",
        "- Sea $[X,Y]$ un dataset supervisado que contiene $m$ ejemplos, cada uno de dimensión $n_x$, entonces $X_{(n_x, m)}$, $Y_{(1, m)}$.\n",
        "\n",
        "<img align=\"center\" src=\"https://github.com/psatencio/intro_keras/blob/master/figures/dataset.png?raw=true\" width=\"500\"/>\n",
        "\n",
        "- Sea $l$ el número de capas de la red y $n^{[i]}$ el numero de neuronas de la capa $i$.\n",
        "- Sea $W^{[i]}$ la matriz de pesos de la capa $i$, entonces $W^{[i]}_{(n^{[i]},n^{[i-1]})}$. Nótese que esto es equivalente a definir $W$ de forma traspuesta.\n",
        "- Sea $b^{[i]}$ el arreglo de bias de la capa $i$, entonces $b^{[i]}_{(n^{[i]}, 1)}$.\n",
        "- Sea $Z^{[i]}$ la activacion lineal o entrada de la capa $i$, entonces $Z^{[i]}_{(n^{[i]}, m)}$\n",
        "- Sea $A^{[i]}$ la salida de la capa $i$, entonces $A^{[i]}_{(n^{[i]}, m)}$. Nótese que $A^{[0]} = X$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zcx6B5kEdoCB",
        "colab_type": "text"
      },
      "source": [
        "<hr>\n",
        "\n",
        "## 3.2. Forward Propagation\n",
        "\n",
        "Este es el proceso de computar la salida de la red a partir de la entrada, en otras palabras, calcular las predicciones para un conjunto de entrada.\n",
        "\n",
        "Este proceso se utiliza principalmente en dos momentos:\n",
        "\n",
        "1. En el entrenamiento de la red.\n",
        "2. Una vez entrenada, para lanzar predicciones.\n",
        "\n",
        "Se define de forma general mediante las siguientes instrucciones:\n",
        "\n",
        "<br>\n",
        "\n",
        "<font size=3>\n",
        "$\n",
        "\\\\\n",
        "para(i:1 \\rightarrow l)\\{\n",
        "\\\\\n",
        "\\hspace{10mm} Z^{[i]} = W^{[i]}.A^{[i-1]} + b^{[i]}\n",
        "\\\\\n",
        "\\hspace{10mm} A^{[i]} = f(A^{[i]})\n",
        "\\\\\n",
        "\\}\n",
        "$\n",
        "</font>\n",
        "\n",
        "Implementemos lo anterior para el caso del regresor en cadena que utilizamos para el XOR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2omkDVLdoCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "Y = np.array([[0, 1, 1, 0]])\n",
        "\n",
        "color= ['blue' if y == 1 else 'red' for y in np.squeeze(Y)]\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.scatter(X[:,0], X[:,1], color=color)\n",
        "plt.grid()\n",
        "plt.xlabel(r'$p$', fontsize=12)\n",
        "plt.ylabel(r'$q$', fontsize=12)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "X = X.T #Transponemos X para que quede de dimension (nx, m)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx8GPAGsdoCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Definamos los W en cada caso\n",
        "np.random.seed(2)\n",
        "W1 = np.random.random([2,2])\n",
        "b1 = np.zeros([2,1])\n",
        "W2 = np.random.random([1,2])\n",
        "b2 = np.zeros([1,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt0zPdD9doCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(Z):\n",
        "    A = 1. / (1. + np.exp(-Z))  \n",
        "    return A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugsaDT8LdoCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Feed-Forward: implementamos el ciclo explicitamente\n",
        "Z1 = np.dot(W1,X) + b1\n",
        "A1 = sigmoid(Z1)\n",
        "Z2 = np.dot(W2, A1) + b2\n",
        "A2 = sigmoid(Z2)\n",
        "\n",
        "print(\"Salida de la red para 4 datos: \", A2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvBwtlm0doCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Este proceso se puede automatizar utilizando alguna estructura de datos, por ejemplo, diccionarios\n",
        "\n",
        "layers = {\"layer1\":{\"W\":W1, \"b\":b1}, \"layer2\":{\"W\":W2, \"b\":b2}} #parametros de la red neuronal\n",
        "pred = {\"layer0\":{\"A\":X}} #diccionario para guardar los Z y A en cada iteracion del feed-forward\n",
        "\n",
        "for i in range(1, len(layers)+1):\n",
        "    layer = layers[\"layer\"+str(i)]\n",
        "    pred[\"layer\"+str(i)] = {}\n",
        "    \n",
        "    #Recuperamos datos de la iteracion anterior\n",
        "    W = layer[\"W\"]\n",
        "    b = layer[\"b\"]\n",
        "    A_prev = pred[\"layer\"+str(i-1)][\"A\"]\n",
        "    \n",
        "    #Feed-forward\n",
        "    Z = np.dot(W, A_prev) + b\n",
        "    A = sigmoid(Z)\n",
        "    \n",
        "    #Salvamos los datos de esta iteracion\n",
        "    pred[\"layer\"+str(i)][\"Z\"] = Z\n",
        "    pred[\"layer\"+str(i)][\"A\"] = A\n",
        "\n",
        "print(\"Salida de la red para 4 datos: \", pred[\"layer2\"][\"A\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eWjfn7FdoCr",
        "colab_type": "text"
      },
      "source": [
        "<hr>\n",
        "\n",
        "## 3.3. Backpropagation + update\n",
        "\n",
        "Backpropagation o retro-propagación del error consiste en aplicar el descenso del gradiente sobre una red neuronal con una __arquitectura arbitraria__, utilizando para ello la regla de la cadena sobre el gráfo de cómputo de la red para obtener las derivadas parciales de cada parámetro a partir del error en la salida.\n",
        "\n",
        "<img align=\"center\" src=\"https://github.com/psatencio/intro_keras/blob/master/figures/backprop_general.png?raw=true\" width=\"500\"/>\n",
        "\n",
        "Se define de forma general mediante las siguientes instrucciones:\n",
        "\n",
        "<br>\n",
        "\n",
        "<font size=3>\n",
        "$\n",
        "\\\\\n",
        "para(i:l \\rightarrow 1)\\{\n",
        "\\\\\n",
        "\\hspace{10mm} dZ^{[i]} = \\left\\{\\begin{matrix} si(i==l) & A^{[i]} - Y \\\\ sino & W^{[i+1]T}.dZ^{[i+1]} * f'({Z^{[i]}})\\end{matrix}\\right.\n",
        "\\\\\n",
        "\\hspace{10mm} dW^{[i]} = \\frac{(dZ^{[i]}.A^{[i-1]T})}{m}\n",
        "\\\\\n",
        "\\hspace{10mm} db^{[i]} = \\frac{\\sum{(dZ^{[i]})}}{m}\n",
        "\\\\\n",
        "\\}\n",
        "$\n",
        "<\\font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYKuNXV6doCu",
        "colab_type": "text"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Ciclo completo\n",
        "\n",
        "Finalmente, el proceso completo de entrenamiento de una red consiste en la secuencia __forward-propagation $\\rightarrow$ back-propagation $\\rightarrow$ actualizacion__:\n",
        "\n",
        "<br>\n",
        "\n",
        "<font size=3>\n",
        "$\n",
        "\\\\\n",
        "para(i:1 \\rightarrow l)\\{\n",
        "\\\\\n",
        "\\hspace{10mm} Z^{[i]} = W^{[i]}.A^{[i-1]} + b^{[i]}\n",
        "\\\\\n",
        "\\hspace{10mm} A^{[i]} = f(A^{[i]})\n",
        "\\\\\n",
        "\\}\n",
        "$\n",
        "<br>\n",
        "<br>\n",
        "$\n",
        "\\\\\n",
        "para(i:l \\rightarrow 1)\\{\n",
        "\\\\\n",
        "\\hspace{10mm} dZ^{[i]} = \\left\\{\\begin{matrix} si(i==l) & A^{[i]} - Y \\\\ sino & W^{[i+1]T}.dZ^{[i+1]} * f'({Z^{[i]}})\\end{matrix}\\right.\n",
        "\\\\\n",
        "\\hspace{10mm} dW^{[i]} = \\frac{(dZ^{[i]}.A^{[i-1]T})}{m}\n",
        "\\\\\n",
        "\\hspace{10mm} db^{[i]} = \\frac{\\sum{(dZ^{[i]})}}{m}\n",
        "\\\\\n",
        "\\}\n",
        "$\n",
        "<br>\n",
        "<br>\n",
        "$\n",
        "\\\\\n",
        "para(i:1 \\rightarrow l)\\{\n",
        "\\\\\n",
        "\\hspace{10mm} W^{[i]} = W^{[i]} - \\alpha dW^{[i]}\n",
        "\\\\\n",
        "\\hspace{10mm} b^{[i]} = b^{[i]} - \\alpha db^{[i]}\n",
        "\\\\\n",
        "\\}\n",
        "$\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLSUQ4ZsdoCx",
        "colab_type": "text"
      },
      "source": [
        "<hr>\n",
        "\n",
        "## 3.4. Ejemplo completo: XOR\n",
        "\n",
        "Implementemos todo el proceso de entrenamiento para el problema del XOR y utilizando las definiciones anteriores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEfTZvOMdoCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(Z):\n",
        "    A = 1. / (1. + np.exp(-Z))  \n",
        "    return A\n",
        "\n",
        "def d_sigmoid(Z):\n",
        "    Ap = sigmoid(Z)*(1-sigmoid(Z))\n",
        "    return Ap\n",
        "\n",
        "def loss(Y, A):\n",
        "    return -(Y * np.log(A) + (1-Y) * np.log(1-A))\n",
        "\n",
        "def cost(l):\n",
        "    return np.mean(l)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHgv3IaIdoC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dataset\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=float)\n",
        "Y = np.array([[0, 1, 1, 0]])\n",
        "X = X.T\n",
        "\n",
        "m = len(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCbon8l0doDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Parametros de la red\n",
        "np.random.seed(2)\n",
        "W1 = np.random.random([2,2])\n",
        "b1 = np.zeros([2,1])\n",
        "W2 = np.random.random([1,2])\n",
        "b2 = np.zeros([1,1])\n",
        "\n",
        "'''\n",
        "Parametros de aprendizaje\n",
        "'''\n",
        "num_epochs = 1000\n",
        "learning_rate = 0.4\n",
        "\n",
        "history = []\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    '''\n",
        "    Forward Propagation\n",
        "    '''\n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = sigmoid(Z1)\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "\n",
        "    '''\n",
        "    Backward Propagation\n",
        "    '''\n",
        "    dZ2 = A2 - Y\n",
        "    dW2 = np.dot(dZ2, A1.T) / m\n",
        "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
        "\n",
        "    dZ1 = np.multiply(np.dot(W2.T, dZ2), d_sigmoid(Z1))\n",
        "    dW1 = np.dot(dZ1, X.T) / m\n",
        "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
        "\n",
        "    '''\n",
        "    Actualizacion de parametros\n",
        "    '''\n",
        "    W1 -= learning_rate * dW1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b1 -= learning_rate * db1\n",
        "    b2 -= learning_rate * db2\n",
        "\n",
        "    '''\n",
        "    Costo\n",
        "    '''\n",
        "    J = cost(loss(Y,A2))\n",
        "    history.append(J)\n",
        "        \n",
        "print(\"parametros actualizados: \", (W1, W2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhfknSVMdoDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_multilayer(W1,b1,W2,b2,X):\n",
        "    Z1 = np.dot(W1,X)+b1\n",
        "    A1 = sigmoid(Z1)\n",
        "    \n",
        "    Z2 = np.dot(W2, A1)+b2\n",
        "    A2 = sigmoid(Z2)\n",
        "\n",
        "    return A2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EMRSR51doDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Visualizacion del error por epoca\n",
        "'''\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(np.linspace(0,num_epochs-1, num_epochs), history)\n",
        "plt.xlabel(\"numero de epocas\")\n",
        "plt.ylabel(\"error: \"+r'$J$')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MH9nJCFdoDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualize_lr(W1,b1,W2,b2,X,Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnQWBaphdoDV",
        "colab_type": "text"
      },
      "source": [
        "<hr>\n",
        "\n",
        "<font size=4>\n",
        "    \n",
        "__Trabajemos__: \n",
        "\n",
        "Utilicemos lo implementado al momento para clasificar el problema de la siguiente figura y pruebe:\n",
        "\n",
        "- Pruebe agregando más neuronas en la capa 1.\n",
        "- Pruebe agregando más capas a la red.\n",
        "- Pruebe distintas configuraciones $\\alpha$.\n",
        "- Pruebe distintos valores para el número de épocas.\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yax81cWidoDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, Y = generate_data('moons', 0.1)\n",
        "Y = Y.reshape(1,len(Y))\n",
        "\n",
        "color = ['blue' if y == 1 else 'red' for y in np.squeeze(Y)] # una lista para darle color a las clases\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(X[:,0], X[:,1], color=color)\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.grid()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKve3B9TX-qp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}